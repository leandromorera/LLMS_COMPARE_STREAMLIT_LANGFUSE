<img width="1898" height="1012" alt="image" src="https://github.com/user-attachments/assets/253c7530-8de8-41e1-bd0d-024b5bc8ebe7" />


# LLM-Judge Fake Receipt Detector

A forensic document analysis system that uses multiple OpenAI vision models as
independent judges to classify receipt images as **FAKE**, **REAL**, or
**UNCERTAIN**. Includes a full-featured dashboard, dataset statistics, batch
evaluation, and comprehensive observability via Langfuse.

---

## Table of Contents

1. [Overview](#overview)
2. [Architecture](#architecture)
3. [How It Works](#how-it-works)
4. [Project Structure](#project-structure)
5. [Quick Start](#quick-start)
6. [Configuration](#configuration)
7. [Ways to Run](#ways-to-run)
8. [Dashboard Guide](#dashboard-guide)
9. [Dataset Management](#dataset-management)
10. [Langfuse Observability](#langfuse-observability)
11. [Test Suite](#test-suite)
12. [Judge Output Schema](#judge-output-schema)
13. [Security Notes](#security-notes)

---

## Overview

The system answers one question per receipt image: **is this receipt forged?**

It does so by:

- Extracting lightweight image features (blur, brightness, dimensions, OCR total)
- Sending the image to **3 OpenAI vision models** acting as forensic judges with
  different personas (strict / balanced / lenient)
- Aggregating the three verdicts via **majority vote**
- Logging every step â€” prompts, completions, token usage, latency, scores â€” to
  **Langfuse** via a local MCP server

---

## Dataset Analisis

- Image dimensions and file size distributions.
- Aspect ratio distributions
- Sharpnes, Brightness and Contrast distributions

<img width="1895" height="993" alt="image" src="https://github.com/user-attachments/assets/15a9b5b7-304a-4068-8589-8c827f871e36" />

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         INPUT LAYER                             â”‚
â”‚  Receipt image (.png/.jpg)   +   OCR text (.txt, optional)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    FEATURE EXTRACTION                           â”‚
â”‚                                                                 â”‚
â”‚  image_basic_stats()         blur_variance_of_laplacian()       â”‚
â”‚  â†’ width, height,            â†’ sharpness proxy                  â”‚
â”‚    aspect_ratio, file_kb     (cv2 Laplacian variance)           â”‚
â”‚                                                                 â”‚
â”‚  brightness_contrast()       extract_total_from_text()          â”‚
â”‚  â†’ mean brightness,          â†’ parsed receipt total             â”‚
â”‚    contrast std              (keyword + next-line search)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      JUDGE PANEL  (3Ã— parallel)                 â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚    judge_1      â”‚  â”‚    judge_2      â”‚  â”‚    judge_3      â”‚ â”‚
â”‚  â”‚  gpt-4o-mini    â”‚  â”‚  gpt-4.1-mini   â”‚  â”‚    gpt-4o       â”‚ â”‚
â”‚  â”‚  temp=0.2       â”‚  â”‚  temp=0.4       â”‚  â”‚  temp=0.7       â”‚ â”‚
â”‚  â”‚  strict /       â”‚  â”‚  balanced /     â”‚  â”‚  lenient /      â”‚ â”‚
â”‚  â”‚  skeptical      â”‚  â”‚  artifact-aware â”‚  â”‚  benefit-of-    â”‚ â”‚
â”‚  â”‚                 â”‚  â”‚                 â”‚  â”‚  the-doubt      â”‚ â”‚
â”‚  â”‚ â†’ label         â”‚  â”‚ â†’ label         â”‚  â”‚ â†’ label         â”‚ â”‚
â”‚  â”‚ â†’ confidence    â”‚  â”‚ â†’ confidence    â”‚  â”‚ â†’ confidence    â”‚ â”‚
â”‚  â”‚ â†’ reasons[]     â”‚  â”‚ â†’ reasons[]     â”‚  â”‚ â†’ reasons[]     â”‚ â”‚
â”‚  â”‚ â†’ flags[]       â”‚  â”‚ â†’ flags[]       â”‚  â”‚ â†’ flags[]       â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚                    â”‚                    â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      MAJORITY VOTE                              â”‚
â”‚                                                                 â”‚
â”‚   Needs â‰¥ 2 of 3 for FAKE or REAL; otherwise â†’ UNCERTAIN        â”‚
â”‚                                                                 â”‚
â”‚   Examples:                                                     â”‚
â”‚   [FAKE, FAKE, REAL]     â†’ FAKE      (2/3)                      â”‚
â”‚   [REAL, REAL, UNCERTAIN]â†’ REAL      (2/3)                      â”‚
â”‚   [FAKE, REAL, UNCERTAIN]â†’ UNCERTAIN (split)                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚                    â”‚
          â–¼                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   RESULT CACHE   â”‚  â”‚           LANGFUSE  (via MCP)            â”‚
â”‚                  â”‚  â”‚                                          â”‚
â”‚ data/            â”‚  â”‚  Trace â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  eval_cache.json â”‚  â”‚    â”œâ”€ span: dataset_analysis          â”‚  â”‚
â”‚                  â”‚  â”‚    â”œâ”€ generation: judge_1             â”‚  â”‚
â”‚  Persists across â”‚  â”‚    â”œâ”€ generation: judge_2             â”‚  â”‚
â”‚  sessions; shown â”‚  â”‚    â”œâ”€ generation: judge_3             â”‚  â”‚
â”‚  in Browse tab   â”‚  â”‚    â”œâ”€ span: vote_aggregation          â”‚  â”‚
â”‚                  â”‚  â”‚    â””â”€ scores (8Ã—):                    â”‚  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚        judge_N_confidence (0â€“100)     â”‚  â”‚
                       â”‚        judge_N_correctness (0/0.5/1)  â”‚  â”‚
                       â”‚        final_correct  (0 or 1)        â”‚  â”‚
                       â”‚        inter_judge_agreement (0 or 1) â”‚  â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## How It Works

### 1. Feature extraction (lightweight, CPU-only)

Before any LLM call the system extracts deterministic signals from the image and
optional OCR file:

| Feature | Method | Purpose |
|---|---|---|
| `width`, `height` | PIL `Image.size` | Dimension sanity check |
| `aspect_ratio` | w / h | Long vs short receipt |
| `file_kb` | `stat().st_size` | Compression proxy |
| `blur_variance` | OpenCV Laplacian variance | Sharpness / scan quality |
| `brightness_mean` | Pixel mean / 255 | Exposure level |
| `contrast_std` | Pixel std / 255 | Dynamic range |
| `ocr_total` | Regex keyword+next-line scan | Parsed receipt total |

> OCR is optional. If no `.txt` file exists, `ocr_total` is `null` and the
> judges rely solely on the image.

### 2. Judge prompt

Each judge receives the same base system prompt but with a different **persona**
injected. The persona shifts the judge's prior, giving the ensemble diverse
perspectives:

```
You are an expert forensic document examiner evaluating whether a receipt
image is forged.
Persona: {persona}

Return ONLY valid JSON with this schema:
{
  "label": "FAKE|REAL|UNCERTAIN",
  "confidence": 0-100,
  "reasons": ["short reason 1", "short reason 2"],
  "flags": ["optional tag"]
}
```

The image is sent as a base-64 data URL in the `image_url` content block.
The `response_format: json_object` mode is used where supported; otherwise the
JSON is extracted from free text.


<img width="1849" height="961" alt="image" src="https://github.com/user-attachments/assets/ce1f30be-63cb-4f70-b64d-499aa404ddf7" />

### 3. Majority vote

```
judges = [judge_1.label, judge_2.label, judge_3.label]

FAKE      if count(FAKE)      >= 2
REAL      if count(REAL)      >= 2
UNCERTAIN otherwise (split or all-uncertain)
```

### 4. Scoring (for evaluation runs)

When a ground-truth label is known, eight numeric scores are attached to the
Langfuse trace:

| Score name | Value | Meaning |
|---|---|---|
| `judge_N_confidence` | 0â€“100 | Raw confidence the judge reported |
| `judge_N_correctness` | 1.0 / 0.5 / 0.0 | Correct / uncertain / wrong |
| `final_correct` | 1.0 or 0.0 | Final verdict matches ground truth |
| `inter_judge_agreement` | 1.0 or 0.0 | All three judges agreed |

---

<img width="1849" height="929" alt="image" src="https://github.com/user-attachments/assets/e9f60fc0-933a-4000-9e4e-3e1f273c3b32" />
<img width="1849" height="967" alt="image" src="https://github.com/user-attachments/assets/6781a985-b549-46c8-b74e-372f81dbe390" />


## Project Structure

```
testthekey/
â”‚
â”œâ”€â”€ app.py                          # â˜… Main dashboard (3-tab Streamlit UI)
â”œâ”€â”€ streamlit_app.py                # Legacy single-receipt UI
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ config.py                   # Settings dataclass + env loading
â”‚   â”œâ”€â”€ dataset.py                  # load_label_table, build_records,
â”‚   â”‚                               #   extract_total_from_text, image_basic_stats
â”‚   â”œâ”€â”€ features.py                 # blur_variance_of_laplacian, brightness_contrast
â”‚   â”œâ”€â”€ judges.py                   # JudgeConfig, run_judge, prompt builder
â”‚   â”œâ”€â”€ vote.py                     # majority_vote, vote_tally
â”‚   â”œâ”€â”€ eval.py                     # JudgeResult / AggregatedResult dataclasses
â”‚   â”œâ”€â”€ langfuse_mcp_client.py      # HTTP client for the Langfuse MCP server
â”‚   â””â”€â”€ langfuse_logger.py          # Legacy direct Langfuse SDK logger
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ run_full_eval_langfuse.py   # â˜… Batch eval + full Langfuse logging
â”‚   â”œâ”€â”€ run_eval20.py               # Batch eval on a sampled CSV
â”‚   â”œâ”€â”€ run_one.py                  # Single-receipt CLI run
â”‚   â”œâ”€â”€ sample_eval20.py            # Sample 20 balanced receipts from a dataset
â”‚   â””â”€â”€ summarize_dataset.py        # CLI dataset statistics report
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ conftest.py                 # sys.path setup
â”‚   â”œâ”€â”€ test_vote.py                # 12 unit tests for voting logic
â”‚   â”œâ”€â”€ test_dataset.py             # 22 tests: labels, paths, OCR, image stats
â”‚   â”œâ”€â”€ test_judges.py              # 18 tests: parsing, normalisation (mocked API)
â”‚   â”œâ”€â”€ test_features.py            # 9 tests: blur, brightness
â”‚   â””â”€â”€ test_integration.py         # 8 real OpenAI API tests (auto-skipped if no key)
â”‚
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ 01_dataset_exploration.ipynb
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ sample/
â”‚   â”‚   â”œâ”€â”€ labels.csv              # Ground-truth manifest (image, forged, label)
â”‚   â”‚   â”œâ”€â”€ images/                 # Receipt images (.png)
â”‚   â”‚   â””â”€â”€ ocr/                    # Per-image OCR text files (.txt)
â”‚   â””â”€â”€ eval_cache.json             # â˜… Auto-created: persisted evaluation results
â”‚
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ takehome_instructions.pdf
â”‚
â”œâ”€â”€ .env                            # Local secrets (never commit)
â”œâ”€â”€ .env.example                    # Template
â”œâ”€â”€ .gitignore
â”œâ”€â”€ pytest.ini
â””â”€â”€ requirements.txt
```

---

## Quick Start

### 1. Create and activate a virtual environment

```bash
python -m venv .venv

# Windows
.venv\Scripts\activate

# macOS / Linux
source .venv/bin/activate
```

### 2. Install dependencies

```bash
pip install -r requirements.txt
```

### 3. Configure environment

```bash
cp .env.example .env
```

Edit `.env`:

```env
OPENAI_API_KEY=sk-proj-...          # required

# Optional â€” only needed if using direct SDK logging (legacy streamlit_app.py)
LANGFUSE_PUBLIC_KEY=pk-lf-...
LANGFUSE_SECRET_KEY=sk-lf-...
LANGFUSE_HOST=http://localhost:3000

# Judge models (defaults shown)
JUDGE_MODEL_1=gpt-4o-mini
JUDGE_MODEL_2=gpt-4.1-mini
JUDGE_MODEL_3=gpt-4o
```

> **Langfuse via MCP** (used by `app.py` and `run_full_eval_langfuse.py`) does
> **not** need `LANGFUSE_*` env vars â€” authentication is handled internally by
> the MCP server at `http://localhost:8005/mcp/`.

### 4. Launch the dashboard

```bash
streamlit run app.py
```

---

## Configuration

| Variable | Default | Description |
|---|---|---|
| `OPENAI_API_KEY` | â€” | **Required.** OpenAI API key |
| `JUDGE_MODEL_1` | `gpt-4o-mini` | Model for the strict judge |
| `JUDGE_MODEL_2` | `gpt-4.1-mini` | Model for the balanced judge |
| `JUDGE_MODEL_3` | `gpt-4o` | Model for the lenient judge |
| `OPENAI_TIMEOUT_SECONDS` | `60` | Per-request timeout |
| `LANGFUSE_PUBLIC_KEY` | â€” | Optional (legacy logger only) |
| `LANGFUSE_SECRET_KEY` | â€” | Optional (legacy logger only) |
| `LANGFUSE_HOST` | â€” | Optional (legacy logger only) |

---

## Ways to Run

### A â€” Dashboard (recommended)

```bash
streamlit run app.py
```

Full three-tab UI. See [Dashboard Guide](#dashboard-guide) below.

### B â€” Batch evaluation + full Langfuse logging

```bash
python -m scripts.run_full_eval_langfuse
```

Processes every row in `data/sample/labels.csv`. Logs to Langfuse:

- Prompt version in Prompt Management (`receipt-forensic-judge`)
- Dataset `receipt-detection-sample` with expected outputs
- One trace per receipt with spans, generations, and 8 scores
- A summary event with accuracy, avg confidence, disagreement rate

### C â€” Single receipt (CLI)

```bash
python scripts/run_one.py \
  --image data/sample/images/X00016469622.png \
  --ground_truth FAKE
```

### D â€” Sample + batch evaluate a larger dataset

```bash
# 1. Sample 20 balanced receipts
python scripts/sample_eval20.py \
  --labels data/findit2/train.txt \
  --seed 7 \
  --out_csv eval_samples/eval_20.csv

# 2. Run evaluation on them
python scripts/run_eval20.py \
  --eval_csv eval_samples/eval_20.csv \
  --image_dir data/findit2/images \
  --out_json reports/eval20_results.json
```

### E â€” CLI dataset summary report

```bash
python scripts/summarize_dataset.py \
  --labels data/sample/labels.csv \
  --image_dir data/sample/images \
  --ocr_dir data/sample/ocr \
  --out_dir reports
```

### F â€” Jupyter exploration notebook

```bash
jupyter notebook notebooks/01_dataset_exploration.ipynb
```

Auto-detects the project root regardless of the directory Jupyter is launched
from.

---

## Dashboard Guide

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SIDEBAR                  â”‚  MAIN AREA                          â”‚
â”‚                           â”‚                                     â”‚
â”‚  Data source              â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â—‹ Sample data            â”‚  â”‚  ðŸ“Š Dataset Stats           â”‚   â”‚
â”‚  â—‹ Custom path            â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â”‚
â”‚                           â”‚  â”‚  ðŸ” Browse                  â”‚   â”‚
â”‚  [ðŸ”„ Reload dataset]      â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â”‚
â”‚                           â”‚  â”‚  â–¶  Evaluate                â”‚   â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚  12 receipts loaded        â”‚                                     â”‚
â”‚  ðŸ”´ FAKE: 6  ðŸŸ¢ REAL: 6   â”‚                                     â”‚
â”‚  âœ… Evaluated: 8 / 12     â”‚                                     â”‚
â”‚                           â”‚                                     â”‚
â”‚  Langfuse:                â”‚                                     â”‚
â”‚  http://20.119.121.220:3000â”‚                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Tab: ðŸ“Š Dataset Stats

Displays automatically on load â€” **no API calls, no cost**:

| Section | Charts |
|---|---|
| KPI row | Total receipts Â· FAKE count Â· REAL count Â· With OCR |
| Row 1 | REAL vs FAKE bar chart Â· Receipt totals histogram (by label) |
| Row 2 | Totals by label box plot (are fakes skewed?) Â· File size box plot |
| Row 3 | Resolution scatter (WÃ—H) Â· Blur box plot Â· Brightness box plot Â· Aspect ratio box plot |
| Summary table | `describe()` statistics grouped by label |

### Tab: ðŸ” Browse

```
Filter by label: [All â–¼]    Select receipt: [âœ… X00016469622.png  [GT: FAKE] â–¼]
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â”‚  Receipt image              â”‚  Ground truth: FAKE (red)           â”‚
â”‚                             â”‚                                      â”‚
â”‚  [image displayed here]     â”‚  Image features:                     â”‚
â”‚                             â”‚   dimensions: 461 Ã— 933 px           â”‚
â”‚                             â”‚   aspect_ratio: 0.494                â”‚
â”‚                             â”‚   file_kb: 225.3                     â”‚
â”‚                             â”‚   blur_variance: 2455.4              â”‚
â”‚                             â”‚   brightness: 0.9646                 â”‚
â”‚                             â”‚   ocr_total: 88.91                   â”‚
â”‚                             â”‚                                      â”‚
â”‚                             â”‚  Verdict: UNCERTAIN  âŒ              â”‚
â”‚                             â”‚  FAKE: 1  REAL: 0  UNCERTAIN: 2      â”‚
â”‚                             â”‚  â–¶ Judge details (expandable)        â”‚
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
```

- `âœ…` = already evaluated (result shown instantly from cache)
- `â¬œ` = not yet evaluated ("Go to â–¶ Evaluate" message shown)
- Judge details expand to show per-judge label, confidence, reasons, latency

### Tab: â–¶ Evaluate

```
Total: 12     Evaluated: 8     Pending: 4

Run mode: â— Run missing only   â—‹ Run all (overwrite)

[â–¶  Start evaluation]

â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘  [3/4]  X00016469623.png  (GT=REAL)
âš™ï¸ Running 3 judges on X00016469623.pngâ€¦

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Receipt          â”‚ GT â”‚ Verdict  â”‚ âœ“ â”‚ FAKE â”‚ REAL â”‚ UNCERTAIN â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ X00016469622.png â”‚FAKEâ”‚ UNCERTAINâ”‚ âŒâ”‚  1   â”‚  0   â”‚     2     â”‚
â”‚ X00016469623.png â”‚REALâ”‚ REAL     â”‚ âœ…â”‚  0   â”‚  2   â”‚     1     â”‚
â”‚ ...              â”‚    â”‚          â”‚   â”‚      â”‚      â”‚           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

- Results are saved to `data/eval_cache.json` **after every receipt** â€” safe to
  interrupt and resume
- Langfuse logging happens silently; if the MCP server is unreachable a warning
  is shown but evaluation continues
- After completion: Accuracy Â· Correct count Â· Uncertain/Wrong summary

---

## Dataset Management

### labels.csv format

```csv
image,forged,label
X00016469622.png,1,FAKE
X00016469623.png,0,REAL
your_new_receipt.png,1,FAKE
```

| Column | Values | Notes |
|---|---|---|
| `image` | filename | Must match a file in the images directory |
| `forged` | `1` or `0` | `1` = FAKE, `0` = REAL |
| `label` | `FAKE` / `REAL` | Auto-derived from `forged`; including it is fine |

### Adding new receipts

```
data/sample/
â”œâ”€â”€ images/
â”‚   â”œâ”€â”€ X00016469622.png        â† existing
â”‚   â””â”€â”€ your_new_receipt.png    â† 1. drop image here
â”œâ”€â”€ ocr/
â”‚   â”œâ”€â”€ X00016469622.txt        â† existing
â”‚   â””â”€â”€ your_new_receipt.txt    â† 2. drop OCR text here (optional)
â””â”€â”€ labels.csv                  â† 3. add a row here
```

After adding files:

1. Click **ðŸ”„ Reload dataset** in the sidebar (clears the feature cache)
2. Go to **â–¶ Evaluate** â†’ **Run missing only** â†’ all new receipts are evaluated
3. Results appear immediately in **ðŸ” Browse**

> OCR is optional. Without it, `ocr_total` will be `null` but the LLM judges
> still run using the image.

### Using a different dataset

In the sidebar select **Custom path** and provide:

- Labels CSV path (must have `image` and `forged` columns)
- Images directory path
- OCR directory path (optional)

The `scripts/sample_eval20.py` script can sample a balanced subset from any
large dataset with a `train.txt` / CSV format.

---

## Langfuse Observability

The project uses a **local MCP server** at `http://localhost:8005/mcp/` to send
data to the Langfuse instance at `http://20.119.121.220:3000`.

```
Your code                    MCP Server              Langfuse
    â”‚                            â”‚                       â”‚
    â”‚  HTTP POST /mcp/           â”‚                       â”‚
    â”‚  (JSON-RPC 2.0 + SSE)      â”‚                       â”‚
    â”‚â”€â”€langfuse_log_generationâ”€â”€â–¶â”‚                       â”‚
    â”‚                            â”‚â”€â”€Langfuse SDK callâ”€â”€â”€â–¶â”‚
    â”‚                            â”‚                       â”‚  stores trace
    â”‚â—€â”€â”€â”€â”€â”€â”€â”€â”€ result â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚                       â”‚  generation
    â”‚                            â”‚                       â”‚  score
```

<img width="1849" height="975" alt="image" src="https://github.com/user-attachments/assets/e5262000-da4c-42b6-ba9e-98a524bdb1c2" />
<img width="1849" height="863" alt="image" src="https://github.com/user-attachments/assets/3e7f7b3d-05e7-4af1-8e43-94b8c7a2c253" />

<img width="1849" height="975" alt="image" src="https://github.com/user-attachments/assets/1391d2d3-ae71-4f28-a1e2-ab950f95070c" />


### What gets logged per evaluation run

```
Langfuse Prompt Management
â””â”€â”€ receipt-forensic-judge  (versioned text prompt)

Dataset: receipt-detection-sample
â”œâ”€â”€ item: X00016469622.png  (input + expected_output)
â””â”€â”€ item: X00016469623.png

Dataset run: eval-3judges-gpt4o
â”œâ”€â”€ run item â†’ trace be3d81f3â€¦
â””â”€â”€ run item â†’ trace 9c63670aâ€¦

Trace: receipt_eval_X00016469622.png
â”œâ”€â”€ span:       dataset_analysis     â† image stats + OCR features
â”œâ”€â”€ generation: judge_1              â† model Â· prompt Â· response Â· tokens Â· latency
â”œâ”€â”€ generation: judge_2
â”œâ”€â”€ generation: judge_3
â”œâ”€â”€ span:       vote_aggregation     â† labels list â†’ final_label + tally
â””â”€â”€ scores (8):
      judge_1_confidence   Â· judge_1_correctness
      judge_2_confidence   Â· judge_2_correctness
      judge_3_confidence   Â· judge_3_correctness
      final_correct        Â· inter_judge_agreement

Event: eval_summary
â””â”€â”€ accuracy Â· avg_confidence Â· disagreement_rate
```

### MCP client

`src/langfuse_mcp_client.py` provides a thin Python wrapper over the MCP
transport. It handles the `initialize` handshake automatically on first use:

```python
from src.langfuse_mcp_client import LangfuseMCPClient

lf = LangfuseMCPClient()
lf.auth_check()                                  # verify connectivity
lf.log_generation(observation={...}, trace={...})
lf.create_score(trace_id, "accuracy", 0.85)
lf.dataset_add_item(dataset_name=..., input=..., expected_output=...)
```

---

## Test Suite

```
tests/
â”œâ”€â”€ conftest.py           sys.path setup
â”œâ”€â”€ test_vote.py          12 tests â€” majority_vote, vote_tally
â”œâ”€â”€ test_dataset.py       22 tests â€” label loading, path resolution, OCR parsing
â”œâ”€â”€ test_judges.py        18 tests â€” JSON parsing, output normalisation (mocked API)
â”œâ”€â”€ test_features.py       9 tests â€” blur variance, brightness contrast
â””â”€â”€ test_integration.py    8 tests â€” real OpenAI API calls (auto-skipped if no key)
```

### Run all unit tests (no API calls, fast)

```bash
pytest tests/ -v
```

### Run integration tests (calls OpenAI, ~$0.01)

```bash
pytest tests/test_integration.py -v -m integration
```

Integration tests auto-skip when `OPENAI_API_KEY` is not set. They use
`gpt-4o-mini` (cheapest vision model) and verify:

- Schema validity of judge responses
- That a strict judge does not call a known-FAKE receipt REAL
- That a strict judge does not call a known-REAL receipt FAKE
- Full 3-judge pipeline produces a valid tally summing to 3

---

## Judge Output Schema

Every judge call returns a validated, normalised dict:

```json
{
  "label":      "FAKE | REAL | UNCERTAIN",
  "confidence": 85.0,
  "reasons": [
    "Font inconsistency on line items",
    "Total does not match item sum"
  ],
  "flags": ["pixel_artifact", "font_mismatch"]
}
```

| Field | Type | Constraints |
|---|---|---|
| `label` | string | One of `FAKE`, `REAL`, `UNCERTAIN` |
| `confidence` | float | 0â€“100 (clamped); 100 = fully certain |
| `reasons` | list[string] | 1â€“5 short, observable statements |
| `flags` | list[string] | 0â€“8 optional categorical tags |

`_normalize_output()` in `src/judges.py` enforces all constraints and provides
safe defaults if the model returns malformed JSON.

---

## Security Notes

- **Never commit `.env`** â€” it is listed in `.gitignore`
- **Never commit API keys** to version control or chat; rotate immediately if
  exposed
- The `.env.example` file contains only placeholder values and is safe to commit
- The sample receipt images are included for development/testing only

---

## Requirements

```
openai>=1.40.0          LLM calls + structured JSON output
langfuse>=3.0.0,<4.0.0  Direct SDK (legacy logger)
python-dotenv>=1.0.1    .env loading
pandas>=2.1.0           Label tables + dataset stats
numpy>=1.26.0           Numerical operations
matplotlib>=3.8.0       Dashboard charts
Pillow>=10.0.0          Image loading + stats
opencv-python>=4.9.0    Blur detection (Laplacian variance)
streamlit>=1.35.0       Dashboard UI
tqdm>=4.66.0            Progress bars (batch scripts)
pytest>=8.0.0           Test runner
httpx                   MCP HTTP transport (installed with openai)
```
